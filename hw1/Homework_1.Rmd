---
title: "Homework 1"
author: "Christopher Marais & Songzi Wu"
date: '2022-09-20'
output: html_document
---

## Q 1

### **1.1**

We imported the data and split the training, validation and test data.

```{r}
data_df <- read.csv("SML.NN.data.csv")
train_df = data_df[data_df$set == 'train',]
valid_df = data_df[data_df$set == 'valid',]
test_df = data_df[data_df$set == 'test',]
```

Thereafter we constructed the function to get the class probability for a single point estimated as the training data points within a radius from the point.

```{r}

# function to get proportion of class 1 in radius to point x (single)
# output is a float
getClass1Prop <- function(x, r, data=train_df) {
  "Import the data this function is 
  based on before using it. The data should be named data_df
  and contain columns Y, X1, X2, set all in a dataframe.
  x is a vector of length 2
  r is the selected radius
  "
  # calculate the distance between the point and all the data
  x1 = (data$X1-as.numeric(x[1]))^2
  x2 = (data$X2-as.numeric(x[2]))^2
  data$euc_dist = sqrt(rowSums(data.frame(x1,x2)))
  #select points inside radius
  in_r_df = data[data$euc_dist <= r,] 
  if(nrow(in_r_df)==0){
    # return NA if no points within radius
    return(NA) 
  } else {
    # calculate proportion of class 1 values in data
    class_1_prop = sum(in_r_df$Y)/nrow(data)
    return(class_1_prop)
  }
}
```

### **1.2**

To create a function that calculates the miss clasification rates we first created a function that creates a classification based on the class 1 probability estimate produced from the previous function. The class 1 probability estimate can be done with the radius function given in 1.1 or with the K nearest neighbor function (KNN) given in 1.6. The method of choice is chosen based on the *func_type* parameter ("radius"=the radius based technique, "knn"= the nearest neighbors technique) . The *rk* parameter is used to allocate either radius size or number of neighbors based on the class 1 probability estimate technique used. The *t* parameter is used to specify the threshold at which the class probability estimate is seen as a positive class 1 classification. The *data* parameter is used to indicate which data set to use as training data.

```{r}
# get class 1 prediction for points in x (multiple)
# output is a vector of binary predictions
getClass1Prediction <- function(x, rk, t=0.5, data=train_df, func_type="radius"){
  "t is the threshold and should be between 0 and 1."
  pred_vec = c()
  # loop through all points in x
  for(i in 1:nrow(x)){
    x_i = x[c('X1','X2')][i,] # get coordinates
    if(func_type=="radius"){
      class_1_prop = getClass1Prop(x=x_i, r=rk, data=data)
    }else if(func_type=="knn") {
      class_1_prop = getClass1PropKNN(x=x_i, k=rk, data=data)
    }
    
    if(is.na(class_1_prop)){
      pred=NA
    }
    else if(class_1_prop >= t){
      pred=1
    }else{
      pred=0
    }
    pred_vec=c(pred_vec, pred)
  }
  return(pred_vec)
}

```

The function to calculate the miss classification was calculated as 1 - the proportion of correct classifications. This means that any classification that resulted in an NA or unknown was labelled as a miss classification.

```{r}
# function to get the miss classification rate
getMissClassRate <- function(true, pred){
  # make dataframe of predictions and true values
  df = data.frame(true, pred)
  # calculate equality of data
  df$equal = (df$true == df$pred)
  mis_class_rate = 1 - sum(df$equal, na.rm = TRUE)/nrow(df)
  return(mis_class_rate)
}
```

### **1.3**

```{r}
# make plot of coordinates density distribution
plot(train_df[train_df$Y==1,]$X1, 
     train_df[train_df$Y==1,]$X2, 
     main = "Distribution of classes",
     xlab = "X1",
     xlim=c(-1.1, 1.1),
     ylab = "X2",
     ylim=c(-1.1, 1.1),
     pch = 15, 
     col = "blue")
points(train_df[train_df$Y==0,]$X1, train_df[train_df$Y==0,]$X2, pch = 0, col = "blue")
points(valid_df[valid_df$Y==1,]$X1, valid_df[valid_df$Y==1,]$X2, pch = 19, col = "red")
points(valid_df[valid_df$Y==0,]$X1, valid_df[valid_df$Y==0,]$X2, pch = 1, col = "red")
legend("topleft", 
       legend=c("Train class 1", "Train class 0", "Valid class 1", "Valid class 0"), 
       col=c("blue","blue","red","red"), 
       pch=c(15,0,19,1), 
       cex=0.8)
```

From the plot showing the distribution of training and validation data. we can see that both data sets have points that are uniformly distributed and that the class one points are surrounded by class 0 points for both data sets. The class 1 points seem to be placed within a even distribution around the (0,0) midpoint with a radius of approximately 0.25. A circular radius is an inconvenient shape to classify the edge cases correctly considering the distribution of the data for class one to also be circular. With a uniform distribution the radius is more likely to include more class 0 points than class 1 points on the border of the two classes. I will refer to this effect as the eclipsing effect. A smaller radius will reduce the eclipsing effect. The radius should also be large enough to reduce then umber of NA classifications. So my guess would be to have the radius be the same as the the distance to the closest neighbor of the most isolated point in the distribution. This is a computationally heavy calculation to perform so I will just guess from a glance at the data that it should be around 0.1.

### **1.4**

```{r}
# select best r for range of r values
# get class predictions
r_grid = seq(0.01, 0.15, 0.01)
class_pred_lst = lapply(r_grid, 
                        getClass1Prediction, 
                        x=valid_df, 
                        t=0.5, 
                        data=train_df,
                        func_type="radius")
# get miss classification rate
miss_class_lst = lapply(class_pred_lst, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=r_grid, 
      y=miss_class_lst,
      main="miss-classifcation by radius",
     xlab="Radius (r)",
     ylab="Miss classification rate",
     ylim=c(0,1),
     col="blue"
      )
lines(x=r_grid, 
      y=miss_class_lst,
      col="blue")
```

From the plot we can see that the lowest r value is **0.12** with a miss classification rate of **0.395** for the validation data. This is not too far off from my guess of the radius of **0.1**.

```{r}
# get miss classification for test data with r*
getMissClassRate(true = test_df$Y,
                  pred = getClass1Prediction(
                    x=valid_df, 
                    t=0.5,
                    rk=0.12,
                    data=train_df,
                    func_type="radius"
                  ))
```

```{r}
# get miss classification for test data with guessed r
getMissClassRate(true = test_df$Y,
                  pred = getClass1Prediction(
                    x=valid_df, 
                    t=0.5,
                    rk=0.1,
                    data=train_df,
                    func_type="radius"
                  ))
```

We can see on the test data that the r\* based on the training data and validation data has a lower miss classification rate (**0.33**) than my guessed r value's miss classification rate (**0.365**). However, these two are still quite close.

### 1.5

We reduced the number of for loops to one. Which can be found in the *getClass1Prediction* function.

### 1.6

We created a similar function to *getClass1Prop* but this one is based on k nearest neighbors (KNN).

```{r}
# KNN
getClass1PropKNN <- function(x, k, data=train_df) {
  "Import the data this function is 
  based on before using it. The data should be named data_df
  and contain columns Y, X1, X2, set all in a dataframe.
  x is a vector of length 2
  k is the number of neigbours
  "
  # calculate the distance between the point and all the data
  x1 = (data$X1-as.numeric(x[1]))^2
  x2 = (data$X2-as.numeric(x[2]))^2
  data$euc_dist = sqrt(rowSums(data.frame(x1,x2)))
  data = data[order(data$euc_dist),]
  class_1_prop = sum(data$Y[1:k])/k
  return(class_1_prop)
}
```

We used this function to similarly get the most optimal value for K in a range of values with the lowest miss classification rate.

```{r}
# get class predictions
k_grid = seq(1, 15, 1)
class_pred_lstKNN = lapply(k_grid, 
                        getClass1Prediction, 
                        x=valid_df, 
                        t=0.5, 
                        data=train_df,
                        func_type="knn")
# get miss classification rate
miss_class_lstKNN = lapply(class_pred_lstKNN, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=k_grid, 
     y=miss_class_lstKNN,
     main="miss-classifcation by radius",
     xlab="Neighbours (k)",
     ylim = c(0,1),
     ylab="Miss classification rate",
     col="red"
)
lines(x=k_grid, 
      y=miss_class_lstKNN,
      col="red")
```

From the plot we can see that 14 and 15 have the lowest MCR values. We opted to go with 14 as the optimal value as this is less complex (Occam's razor). It is also worth noting that overall the KNN approach has much lower MCR values than any of the radius values tested gave. even the worst value of k has a lower MCR value than the best value of r when tested on the validation data. This is likely because the radius approach creates NA classifications that are registered as miss classifications when we calculate MCR.

```{r}
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
                  pred = getClass1Prediction(
                    x=valid_df, 
                    t=0.5,
                    rk=14,
                    data=train_df,
                    func_type="knn"
                  ))
```

However, when we use the optimal value of k on the test data we get a MCR of **0.465** which is higher than what we got for the optimal value of r (r\*). This shows how these two methods differently describe the neighborhood they use to classify points.
