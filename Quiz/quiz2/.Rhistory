Y = 1 + 3*X -2*X^2 + 1*X^3 + rnorm(n)
fit0 = lm(Y ~ X + I(X^2) + I(X^3))
summary(fit0)
myPolyReg1(Y,X,deg=3)
myAnova1 <- function(Y, XF, is1=TRUE) {
# Inputs: same as for myOLS, except
# * XF is a vector of length n that contain the covariate values (categorical or "factor")
# Outputs: same as for myOLS
uniq_var <- unique(XF)
X <- +outer(XF, uniq_var, `==`)
colnames(X) <- uniq_var
if(is1==FALSE){
X = X[,-1]
}
return(myOLS(X=X, Y=Y, is1=is1))
}
n = 30
set.seed(0)
XF = rep(c("A","B","C"),each=10)
Y = rnorm(n) + rep(c(1,2,3),each=10)
fit1 = lm(Y ~ XF)
summary(fit1) # with an intercept
myAnova1(Y, XF, is1=FALSE)
fit0 = lm(Y ~ -1 + XF)
summary(fit0) # without an intercept
myAnova1(Y, XF, is1=TRUE)
myFullObj <- function(par) {
# Inputs:
# * b is the vector of regression coefficients, b=[b0,b1];
# * sig is the standard dev of errors; sig > 0
# Output: the negative log-likelihood of the observed data (Y given X) evaluated at b and sig
b = par[1:2]
sig = par[3]
miu <- b[1]+mean(X)*b[2]
return((-1)*sum(dnorm(Y, mean=miu, sd = sig, log=TRUE)))
}
sigKnown = 2
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
optim(par=c(1,3),myObj1,method="BFGS")
optim(par = c(1,3,1), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
lm(Y ~ X)
summary(lm(Y ~ X))$coeff
summary(lm(Y ~ X))$coeff
summary(lm(Y ~ X))
optim(par = c(1,1,1), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,3,1), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(0,0,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
summary(lm(Y ~ X))
myOLS <- function(Y, X, is1 = TRUE) {
# Inputs:
# * Y is the vector of length n of response variables
# * X is an n-by-p matrix of numerical covariates (in columns); p < n
# ** assume the columns of X are linearly independent and
# ** do not include the column for the intercept as a part of the X matrix
# * is1 is a logical "flag" whether the intercept is included; is1 = TRUE by default
# Output:
# the function must return a list L with two elements:
# L[1] will contain the vector of OLS/MLE coefficients, betahat
# L[2] will contain standard errors (i.e., estimated standard deviations) for betahat
n_y = length(Y)
n_x = nrow(X)
if(n_x != n_y){
print("Error: X and Y not of same length")
}else{
n = n_y
}
p = ncol(X)
if(is1==FALSE){
# add intercept to matrix
X0 = rep(1, n)
X = cbind(X0, X)
}
betahat = solve(t(X)%*%X)%*%t(X)%*%Y
pred = X%*%betahat # prediction
sigma_sq <- sum((Y - pred)^2)/(n-p)  # estimate of sigma-squared
var_covar <- sigma_sq*solve(t(X)%*%X) # variance covariance matrix
std_err <- sqrt(diag(var_covar)) # standard error
return(list(betahat, std_err))
}
n = 30
set.seed(0)
p = 3
X = matrix(runif(n*p),nrow=n)*2-1
b = seq(1,p,by=1)
Y = X%*%b + rnorm(n)
fit1 = lm(Y ~ X); summary(fit1)
myOLS(Y,X,FALSE)
myPolyReg1 <- function(Y, X1, deg=1) {
# Inputs: same as for myOLS, except
# * X1 is a vector of length n that contain the covariate values (numerical)
# * deg is the degree k (i.e., largest power) of the polynomial fit; k < n ; deg=1 by default.
# Outputs: same as for myOLS
X = rep(1, n)
for (i in seq(1, deg)){
X = cbind(X, X1**i)
}
return(myOLS(X=X, Y=Y, is1=TRUE))
}
n = 30
set.seed(0)
X = runif(n)*4-2 # X is uniformly distributed on [-2,2]
Y = 1 + 3*X -2*X^2 + 1*X^3 + rnorm(n)
fit0 = lm(Y ~ X + I(X^2) + I(X^3))
summary(fit0)
myPolyReg1(Y,X,deg=3)
myAnova1 <- function(Y, XF, is1=TRUE) {
# Inputs: same as for myOLS, except
# * XF is a vector of length n that contain the covariate values (categorical or "factor")
# Outputs: same as for myOLS
uniq_var <- unique(XF)
X <- +outer(XF, uniq_var, `==`)
colnames(X) <- uniq_var
if(is1==FALSE){
X = X[,-1]
}
return(myOLS(X=X, Y=Y, is1=is1))
}
n = 30
set.seed(0)
XF = rep(c("A","B","C"),each=10)
Y = rnorm(n) + rep(c(1,2,3),each=10)
fit1 = lm(Y ~ XF)
summary(fit1) # with an intercept
myAnova1(Y, XF, is1=FALSE)
fit0 = lm(Y ~ -1 + XF)
summary(fit0) # without an intercept
myAnova1(Y, XF, is1=TRUE)
n = 30; set.seed(0);
X = runif(n)*4-2;
Y = 1 + 3*X + rnorm(n);
fit1 = lm(Y ~ X); summary(fit1) # beta0_hat = 1.0161; beta1_hat = 2.9304
myFullObj <- function(par) {
# Inputs:
# * b is the vector of regression coefficients, b=[b0,b1];
# * sig is the standard dev of errors; sig > 0
# Output: the negative log-likelihood of the observed data (Y given X) evaluated at b and sig
b = par[1:2]
sig = par[3]
miu <- b[1]+mean(X)*b[2]
return((-1)*sum(dnorm(Y, mean=miu, sd = sig, log=TRUE)))
}
sigKnown = 2
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
optim(par=c(1,3),myObj1,method="BFGS")
optim(par = c(0,0,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
summary(lm(Y ~ X))
optim(par = c(1,3,1), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
for i in c(-2,1,0,1,2){
for( i in c(-2,1,0,1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
optim(par=c(1,3),myObj1,method="BFGS")
}
for( i in c(0,1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
optim(par=c(1,3),myObj1,method="BFGS")
}
for( i in c(1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
optim(par=c(1,3),myObj1,method="BFGS")
}
for( i in c(1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
optim(par=c(1,3),myObj1,method="BFGS")
}
for( i in c(1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(optim(par=c(1,3),myObj1,method="BFGS"))
}
for( i in c(1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print("Sigknown = ", i)
print(optim(par=c(1,3),myObj1,method="BFGS"))
}
print(c("Sigknown = ", i))
for(i in c(1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
}
for(i in c(0, 1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
}
for(i in c(0.1,1,2)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
}
for(i in c(0.1,1,10,100)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
}
for(i in c(0.1,1,10,100)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
print("\n")
}
for(i in c(0.1,1,10,100)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
print()
}
for(i in c(0.1,1,10,100)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
print("----------------")
}
for(i in c(2,0.1,1,10,100)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
print("----------------")
}
optim(par = c(0,0,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(0,1,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(0,3,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(0,0,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,3,1), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,3,10), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,3,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,5,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,2,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,0,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(0,0,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(0,5,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(2,5,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(2,5,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(20,5,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(100,100,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
optim(par = c(1,3,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
1.2486 + 1.6770
sqrt(3.563113)
3.563113^2
x1 <- c(6,2,9,1)
x2 <- c(2,4,1,2)
x3 <- c(4,-2,8,-1)
m1 <- cbind(x1,x2,x3)
lmod <- lm(x1~x3+x2)
summary(lmod)
m2 <- matrix(rnorm(10000* 2), # Create random matrix
ncol = 2)
cor(m2[,1],m2[,2])
myOLS <- function(Y, X, is1 = TRUE) {
# Inputs:
# * Y is the vector of length n of response variables
# * X is an n-by-p matrix of numerical covariates (in columns); p < n
# ** assume the columns of X are linearly independent and
# ** do not include the column for the intercept as a part of the X matrix
# * is1 is a logical "flag" whether the intercept is included; is1 = TRUE by default
# Output:
# the function must return a list L with two elements:
# L[1] will contain the vector of OLS/MLE coefficients, betahat
# L[2] will contain standard errors (i.e., estimated standard deviations) for betahat
n_y = length(Y)
n_x = nrow(X)
if(n_x != n_y){
print("Error: X and Y not of same length")
}else{
n = n_y
}
p = ncol(X)
if(is1==FALSE){
# add intercept to matrix
X0 = rep(1, n)
X = cbind(X0, X)
}
betahat = solve(t(X)%*%X)%*%t(X)%*%Y
pred = X%*%betahat # prediction
sigma_sq <- sum((Y - pred)^2)/(n-p)  # estimate of sigma-squared
var_covar <- sigma_sq*solve(t(X)%*%X) # variance covariance matrix
std_err <- sqrt(diag(var_covar)) # standard error
return(list(betahat, std_err))
}
n = 30
set.seed(0)
p = 3
X = matrix(runif(n*p),nrow=n)*2-1
b = seq(1,p,by=1)
Y = X%*%b + rnorm(n)
fit1 = lm(Y ~ X); summary(fit1)
myOLS(Y,X,FALSE)
myPolyReg1 <- function(Y, X1, deg=1) {
# Inputs: same as for myOLS, except
# * X1 is a vector of length n that contain the covariate values (numerical)
# * deg is the degree k (i.e., largest power) of the polynomial fit; k < n ; deg=1 by default.
# Outputs: same as for myOLS
X = rep(1, n)
for (i in seq(1, deg)){
X = cbind(X, X1**i)
}
return(myOLS(X=X, Y=Y, is1=TRUE))
}
n = 30
set.seed(0)
X = runif(n)*4-2 # X is uniformly distributed on [-2,2]
Y = 1 + 3*X -2*X^2 + 1*X^3 + rnorm(n)
fit0 = lm(Y ~ X + I(X^2) + I(X^3))
summary(fit0)
myPolyReg1(Y,X,deg=3)
myAnova1 <- function(Y, XF, is1=TRUE) {
# Inputs: same as for myOLS, except
# * XF is a vector of length n that contain the covariate values (categorical or "factor")
# Outputs: same as for myOLS
uniq_var <- unique(XF)
X <- +outer(XF, uniq_var, `==`)
colnames(X) <- uniq_var
if(is1==FALSE){
X = X[,-1]
}
return(myOLS(X=X, Y=Y, is1=is1))
}
n = 30
set.seed(0)
XF = rep(c("A","B","C"),each=10)
Y = rnorm(n) + rep(c(1,2,3),each=10)
fit1 = lm(Y ~ XF)
summary(fit1) # with an intercept
myAnova1(Y, XF, is1=FALSE)
fit0 = lm(Y ~ -1 + XF)
summary(fit0) # without an intercept
myAnova1(Y, XF, is1=TRUE)
n = 30
set.seed(0)
X = runif(n)*4-2
Y = 1 + 3*X + rnorm(n)
fit1 = lm(Y ~ X)
summary(fit1) # beta0_hat = 1.0161; beta1_hat = 2.9304
myFullObj <- function(par) {
# Inputs:
# * b is the vector of regression coefficients, b=[b0,b1];
# * sig is the standard dev of errors; sig > 0
# Output: the negative log-likelihood of the observed data (Y given X) evaluated at b and sig
b = par[1:2]
sig = par[3]
miu <- b[1]+mean(X)*b[2]
return((-1)*sum(dnorm(Y, mean=miu, sd = sig, log=TRUE)))
}
for(i in c(2,0.1,1,10,100)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
print("----------------")
}
optim(par = c(1,3,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
x1 <- c(6,2,9,1)
x2 <- c(2,4,1,2)
x3 <- c(4,-2,8,-1)
m1 <- cbind(x1,x2,x3)
lmod <- lm(x1~x3+x2)
summary(lmod)
m2 <- matrix(rnorm(10000* 2), # Create random matrix
ncol = 2)
cor(m2[,1],m2[,2])
myOLS <- function(Y, X, is1 = TRUE) {
# Inputs:
# * Y is the vector of length n of response variables
# * X is an n-by-p matrix of numerical covariates (in columns); p < n
# ** assume the columns of X are linearly independent and
# ** do not include the column for the intercept as a part of the X matrix
# * is1 is a logical "flag" whether the intercept is included; is1 = TRUE by default
# Output:
# the function must return a list L with two elements:
# L[1] will contain the vector of OLS/MLE coefficients, betahat
# L[2] will contain standard errors (i.e., estimated standard deviations) for betahat
n_y = length(Y)
n_x = nrow(X)
if(n_x != n_y){
print("Error: X and Y not of same length")
}else{
n = n_y
}
p = ncol(X)
if(is1==FALSE){
# add intercept to matrix
X0 = rep(1, n)
X = cbind(X0, X)
}
betahat = solve(t(X)%*%X)%*%t(X)%*%Y
pred = X%*%betahat # prediction
sigma_sq <- sum((Y - pred)^2)/(n-p)  # estimate of sigma-squared
var_covar <- sigma_sq*solve(t(X)%*%X) # variance covariance matrix
std_err <- sqrt(diag(var_covar)) # standard error
return(list(betahat, std_err))
}
n = 30
set.seed(0)
p = 3
X = matrix(runif(n*p),nrow=n)*2-1
b = seq(1,p,by=1)
Y = X%*%b + rnorm(n)
fit1 = lm(Y ~ X); summary(fit1)
myOLS(Y,X,FALSE)
myPolyReg1 <- function(Y, X1, deg=1) {
# Inputs: same as for myOLS, except
# * X1 is a vector of length n that contain the covariate values (numerical)
# * deg is the degree k (i.e., largest power) of the polynomial fit; k < n ; deg=1 by default.
# Outputs: same as for myOLS
X = rep(1, n)
for (i in seq(1, deg)){
X = cbind(X, X1**i)
}
return(myOLS(X=X, Y=Y, is1=TRUE))
}
n = 30
set.seed(0)
X = runif(n)*4-2 # X is uniformly distributed on [-2,2]
Y = 1 + 3*X -2*X^2 + 1*X^3 + rnorm(n)
fit0 = lm(Y ~ X + I(X^2) + I(X^3))
summary(fit0)
myPolyReg1(Y,X,deg=3)
myAnova1 <- function(Y, XF, is1=TRUE) {
# Inputs: same as for myOLS, except
# * XF is a vector of length n that contain the covariate values (categorical or "factor")
# Outputs: same as for myOLS
uniq_var <- unique(XF)
X <- +outer(XF, uniq_var, `==`)
colnames(X) <- uniq_var
if(is1==FALSE){
X = X[,-1]
}
return(myOLS(X=X, Y=Y, is1=is1))
}
n = 30
set.seed(0)
XF = rep(c("A","B","C"),each=10)
Y = rnorm(n) + rep(c(1,2,3),each=10)
fit1 = lm(Y ~ XF)
summary(fit1) # with an intercept
myAnova1(Y, XF, is1=FALSE)
fit0 = lm(Y ~ -1 + XF)
summary(fit0) # without an intercept
myAnova1(Y, XF, is1=TRUE)
n = 30
set.seed(0)
X = runif(n)*4-2
Y = 1 + 3*X + rnorm(n)
fit1 = lm(Y ~ X)
summary(fit1) # beta0_hat = 1.0161; beta1_hat = 2.9304
myFullObj <- function(par) {
# Inputs:
# * b is the vector of regression coefficients, b=[b0,b1];
# * sig is the standard dev of errors; sig > 0
# Output: the negative log-likelihood of the observed data (Y given X) evaluated at b and sig
b = par[1:2]
sig = par[3]
miu <- b[1]+mean(X)*b[2]
return((-1)*sum(dnorm(Y, mean=miu, sd = sig, log=TRUE)))
}
for(i in c(2,0.1,1,10,100)){
sigKnown = i
myObj1 <- function(b){
myFullObj(c(b,sigKnown))
}
print(c("Sigknown = ", i))
print(optim(par=c(1,3),myObj1,method="BFGS"))
print("----------------")
}
optim(par = c(1,3,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
