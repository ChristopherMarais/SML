---
title: "SML HW2"
author: "Christopher Marais"
output: pdf_document
fontsize: 11pt
geometry: margin=2cm
---
## ISLR Chapter 3:
Import libraries
```{r}
library("ISLR2")
```

### Question 4

4.a
With the cubic regression the RSS should be lower. This is because the bonus features in the cubic regression allows it to describe the training data with more precision due to its higher complexity. 


4.b
Because the true relationship is linear we expect the linear regression to perform better than the cubic regression. We expect the linear regression to have a lower test RSS than the cubic regression. The linear model should be able to generalize better than the cubic model. The cubic regression might more easily over fit on the training data. 

4.c
We still expect the cubic regression to have a lower RSS on the training data. The linear regression model will have a higher RSS than in (a.) because the true relationship is non-linear. 

4.d
On the spectrum on linearity if the true relationship is closer to what is described by the cubic regression, the cubic model will have a lower RSS. However, if the model is closer to the linear regression model the linear regression model might have a lower RSS. in thee case where the true relationship is closer to the cubic model in non-linearity it will result in the linear regression model will under fit, whereas if the true relationship is closer to the linear model then it will result in the cubic model to over fit to the training data. 

### Question 10
10.a
```{r}
model1 = lm(Sales~Price+Urban+US,data=Carseats)
summary(model1)
```

10.b
The (intercept) is the number of car seats that would be sold according to the model if all other features were ignored. The Price coefficient indicates that car seats sales will decrease by ~50 for every dollar the price goes increases. The UrbanYes coefficient is not at all significant with a very high p value of 0.936. It is therefore, uninformative. The USYes coefficient indicates that on average ~20% more car seats are sold if the shop is located in the US than if it were outside of the US. 

10.c
```{r}
attach(Carseats)
contrasts(US)
contrasts(Urban)
```
$$Sales = 13.04 - 0.05Price  -0.02UrbanYes + 1.20USYes$$

10.d
```{r}
model2 = lm(Sales~.,data=Carseats)
summary(model2)
```
The null hypothesis can be rejected for all the predictors excluding Population, Education, UrbanYes, USYes. 

10.e
```{r}
model3 = lm(Sales~.-Education-Urban-US-Population,data=Carseats)
summary(model3)
```

10.f
The model in (a) is a worse fit than the one in (e) this is based on the RSE, R2 and F-statistic. This RSE is higher for (a) at 2.47 than for (e) at 1.02. Furthermore, the R2 and F-statistic are both higher for model (e) (0.872 and 381.4 respectively) than for model (a) (0.24 and 41.52 respectively).

10.g
```{r}
confint(model3)
```


## Typed Problem 1

Let $Y_1,...,Y_n$ be iid rvs with $E(Y_i)=a$ and $E(Y_i^2)=b$, so that $Var(Y_i)=b-a^2$.

Define $T = \sum_{i=1}^n (Y_i-\bar{Y})^2$, where $\bar{Y} = n^{-1}\sum_{i=1}^n Y_i$ is the sample mean.

1.2. Suppose we estimate the population variance $Var(Y_i)$ by $cT$ for some constant $c>0$. What value of $c$ results in an unbiased estimator of the population variance?
(The answer you should get is $c=1/(n-1)$.) Let $T_1 = cT$ be this unbiased estimator.


1.3. Let $Y_1,...,Y_n$ be iid $Normal(\mu,\sigma^2)$, where $\mu$ and $\sigma^2$ are the population mean and variance, respectively. 
One can show that $T_2 = T/n$ is the MLE for $\sigma^2$; you can take this fact for granted.

Use R to examine the small-sample properties of $T_1$ and $T_2$ as follows:

(a) Generate the data as follows:

m=1000; n=4; # n is the sample size; m is the # of replications
set.seed(0); 
M = matrix(rnorm(m*n),nrow=m); # default parameters in rnorm are mean=0, sd=1;
M is an m-by-n matrix with replications of the experiment stored in rows

```{r}
m = 1000
n = 4
set.seed(0)
M = matrix(rnorm(m*n),nrow=m)
```


(b) For each row of M, evaluate and store values of $T_1$ and $T_2$, in separate vectors.
(Optional): you can do this without loops using apply() function

```{r}
fun_T1 <- function(x) {
  return((1/(length(x)-1))*sum((x-mean(x))^2))
}

T1 <- apply(M, 1, fun_T1)

fun_T2 <- function(x) {
  return((1/length(x))*sum((x-mean(x))^2))
}

T2 <- apply(M, 1, fun_T2)

```


(c) Plot histograms of $T_1$ and $T_2$.

```{r}
hist(T1)
```


```{r}
hist(T2)
```

(d) "Monte Carlo integration" is estimation of population moments of a rv $X$ by the corresponding sample moments whenever one can simulate iid variates $X_1,X_2,\ldots$ from the sampling distribution of $X$. I.e., using the law of large numbers (and another result known as the continuous mapping theorem) $\bar{X}_n \rightarrow E(X)$ and $S_n^2 \rightarrow Var(X)$ as $n\rightarrow \infty$, where $\bar{X}_n$ and $S_n^2$ are the sample mean and the sample variance, respectively. 
Use "Monte Carlo integration" to estimate bias, variance and MSE of the two estimators. Specifically, you can estimate $E(T_1)$ and $E(T_2)$ using the respective sample means, and (population) variances of $T_1$ and $T_2$ using the sample variances of $T_1$ and $T_2$.

```{r}
mse <- function(true, pred){
  return(mean((true - pred))^2)
}


# mean(T1)
# var(T1)
# T1_bias <- mean(T1)-1
# 
# mean(T2)
# var(T2)
# T2_bias <- mean(T2)-1
mse(true=1, pred=T1)
```
  Briefly discuss your findings in (c) and (d).



1.4. Suppose we are now interested in the population standard deviation, i.e., $\sigma=\sqrt{\sigma^2}$. Explain/argue whether $\sqrt{T_1}$ is unbiased for estimation of $\sigma$, and why. Feel free to extend the simulation study in 1.3 to reinforce your answer.

```{r}

```




