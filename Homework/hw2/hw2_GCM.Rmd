---
title: "SML HW2"
author: "Christopher Marais"
output: pdf_document
fontsize: 11pt
geometry: margin=2cm
---
## ISLR Chapter 3:
Import libraries
```{r}
library("ISLR2")
```

### Question 4

4.a
The extra polynomial terms allow for a closer fit 
(more degrees of freedom) of the training data, so I would 
expect the training RSS for cubic regression to be lower than 
for simple linear regression.
```{r}

```

4.b
The true relationship is linear and so simple linear regression
would generalize better to unseen data, as such I would expect it 
to have lower test RSS. The cubic model likely over fit the training 
data, and so I would expect it to have a higher test RSS.
```{r}

```

4.c
Cubic regression will have a better fit to non-linear data and so 
its training RSS will be lower.
```{r}

```

4.d
The test RSS depends on how far from linear the true relationship$f(x)$ is.
If $f(x)$ is more linear than cubic, then cubic regression can over fit,
so cubic RSS will be higher and liner RSS will be lower.
If $f(x)$ is more cubic than linear, then linear regression can under fit, 
so linear RSS will be higher and cubic RSS will be lower."
```{r}

```

### Question 10
10.a
```{r}
carseats_lm = lm(Sales~Price+Urban+US,data=Carseats)
summary(carseats_lm)
```

10.b
- The intercept represents the number of car seats sold on average when all other predictors are disregarded.
- The `Price` coefficient is negative and so sales will fall by roughly 54 seats(0.054x1000)for every unit($1) increase in price.
- The `Urban=Yes` coeff is not statistically significant. The `US=Yes` coeff is 1.2, and this means an average increase in car seat sales of 1200 units when `US=Yes`(this predictor likely refers to the shop being in the USA).

```{r}

```

10.c
```{r}
attach(Carseats)
contrasts(US)
contrasts(Urban)
```
$$Sales = 13.04\ + -0.05Price \ + -0.02Urban(Yes:1,No:0) \ + 1.20US(Yes:1,No:0)$$

10.d
- Null hypothesis can be rejected for `CompPrice`, `Income`, `Advertising`, `Price`, `ShelvelocGood`, `ShelvelocMedium` and `Age`.

```{r}
carseats_all_lm = lm(Sales~.,data=Carseats)
summary(carseats_all_lm)
```

10.e
```{r}
carseats_all_lm2 = lm(Sales~.-Education-Urban-US-Population,data=Carseats)
summary(carseats_all_lm2)
```

10.f
- The RSE goes down from 2.47 __model (a)__ to 1.02 __model (e)__. The R2 statistic goes up from 0.24 __(a)__ to 0.872 __(e)__ and the F-statistic goes up from 41.52 to 381.4. 
- The statistical evidence clearly shows that __(e)__ is a much better fit.
```{r}

```

10.g
```{r}
confint(carseats_all_lm2)
```


## Typed Problem 1

Let $Y_1,...,Y_n$ be iid rvs with $E(Y_i)=a$ and $E(Y_i^2)=b$, so that $Var(Y_i)=b-a^2$.

Define $T = \sum_{i=1}^n (Y_i-\bar{Y})^2$, where $\bar{Y} = n^{-1}\sum_{i=1}^n Y_i$ is the sample mean.

1.1. (Optional) 
Use the properties/calculus of expectations to find $E(T)$.
If you are not able to find $E(T)$, you can use use $E(T) = (n-1)Var(Y_i)$ in subsequent subproblems.

1.2. Suppose we estimate the population variance $Var(Y_i)$ by $cT$ for some constant $c>0$. What value of $c$ results in an unbiased estimator of the population variance? 
(The answer you should get is $c=1/(n-1)$.) Let $T_1 = cT$ be this unbiased estimator.

1.3. Let $Y_1,...,Y_n$ be iid $Normal(\mu,\sigma^2)$, where $\mu$ and $\sigma^2$ are the population mean and variance, respectively. 
One can show that $T_2 = T/n$ is the MLE for $\sigma^2$; you can take this fact for granted.

Use R to examine the small-sample properties of $T_1$ and $T_2$ as follows:

(a) Generate the data as follows:

m=1000; n=4; # n is the sample size; m is the # of replications
set.seed(0); 
M = matrix(rnorm(m*n),nrow=m); # default parameters in rnorm are mean=0, sd=1;
M is an m-by-n matrix with replications of the experiment stored in rows


(b) For each row of M, evaluate and store values of $T_1$ and $T_2$, in separate vectors.
(Optional): you can do this without loops using apply() function

(c) Plot histograms of $T_1$ and $T_2$.

(d) "Monte Carlo integration" is estimation of population moments of a rv $X$ by the corresponding sample moments whenever one can simulate iid variates $X_1,X_2,\ldots$ from the sampling distribution of $X$. I.e., using the law of large numbers (and another result known as the continuous mapping theorem) $\bar{X}_n \rightarrow E(X)$ and $S_n^2 \rightarrow Var(X)$ as $n\rightarrow \infty$, where $\bar{X}_n$ and $S_n^2$ are the sample mean and the sample variance, respectively. 
Use "Monte Carlo integration" to estimate bias, variance and MSE of the two estimators. Specifically, you can estimate $E(T_1)$ and $E(T_2)$ using the respective sample means, and (population) variances of $T_1$ and $T_2$ using the sample variances of $T_1$ and $T_2$.

  Briefly discuss your findings in (c) and (d).

1.4. Suppose we are now interested in the population standard deviation, i.e., $\sigma=\sqrt{\sigma^2}$. Explain/argue whether $\sqrt{T_1}$ is unbiased for estimation of $\sigma$, and why. Feel free to extend the simulation study in 1.3 to reinforce your answer.



