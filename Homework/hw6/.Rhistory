coef(fit, id=which.min(cp))
# Plot Cp
{plot(cp,
main="Forward selection",
xlab="Predictors",
ylab="Mallows' Cp",
type="l")
abline(v=which.min(cp),
col="red",
lwd=3,
lty=2)}
print(paste("The Cp decreases and then plateaus at the lowest value of",
which.min(cp),
"predictors."))
# Plot BIC
{plot(bic,
main="Forward selection",
xlab="Predictors",
ylab="Bayesian Information Criterion (BIC)",
type="l")
abline(v=which.min(bic),
col="red",
lwd=3,
lty=2)}
print(paste("The BIC decreases and then slowly increases as the number of predictors increases as well with a lowest BIC value at",
which.min(bic),
"predictors."))
# Plot adjr2
{plot(adjusted_r_sq,
main="Forward selection",
xlab="Predictors",
ylab="Adjusted R Squares",
type="l")
abline(v=which.max(adjusted_r_sq),
col="red",
lwd=3,
lty=2)}
print(paste("The Adjusted R Squares increases and then plateaus at the highest value of",
which.max(adjusted_r_sq),
"predictors."))
# specify data
x_mat = data.matrix(df[1:10])
y_mat = data.matrix(Y)
# train/test split
train_idx = sample(1:nrow(x_mat), nrow(x_mat)/2)
test_idx = (-train_idx)
# cross validation
folds_fit = cv.glmnet(x_mat[train_idx,],
y_mat[train_idx,],
alpha = 1,
folds=10)
# plot results
plot(folds_fit)
# Test data MSE
pred = predict(folds_fit,
s = folds_fit$lambda.min,
newx = x_mat[test_idx, ])
test_mse = mean((pred - Y[test_idx])^2)
print(paste("Test MSE: ", test_mse))
print(paste("Mininum lambda: ", folds_fit$lambda.min))
# define new response
b_7 = 13
Y = c(b_0 + b_7*X^7 + e)
# Best subset selection
# Save data in dataframe
df = data.frame(X,X^2,X^3,X^4,X^5,X^6,X^7,X^8,X^9,X^10,Y)
# Find best subsets
fit <- regsubsets(Y ~ ., data = df, nvmax=10)
summary(fit)
# Extract cp, bic and adjr2
cp = summary(fit)$cp
bic = summary(fit)$bic
adjusted_r_sq = summary(fit)$adjr2
# Plot Cp
{plot(cp,
main="Best subset selection",
xlab="Number of predictors",
ylab="Mallows' Cp",
type="l")
abline(v=which.min(cp),
col="red",
lwd=3,
lty=2)}
# Plot BIC
{plot(bic,
main="Best subset selection",
xlab="Number of predictors",
ylab="Bayesian Information Criterion (BIC)",
type="l")
abline(v=which.min(bic),
col="red",
lwd=3,
lty=2)}
# Plot adjr2
{plot(adjusted_r_sq,
main="Best subset selection",
xlab="Number of predictors",
ylab="Adjusted R Squares",
type="l")
abline(v=which.max(adjusted_r_sq),
col="red",
lwd=3,
lty=2)}
# print coefficients
coef(fit, id=which.min(cp))
# LASSO
# specify data
x_mat = data.matrix(df[1:10])
y_mat = data.matrix(Y)
# train/test split
train_idx = sample(1:nrow(x_mat), nrow(x_mat)/2)
test_idx = (-train_idx)
# cross validation
folds_fit = cv.glmnet(x_mat[train_idx,],
y_mat[train_idx,],
alpha = 1,
folds=10)
# plot results
plot(folds_fit)
# Test data MSE
pred = predict(folds_fit,
s = folds_fit$lambda.min,
newx = x_mat[test_idx, ])
test_mse = mean((pred - Y[test_idx])^2)
print(paste("Test MSE: ", test_mse))
print(paste("Mininum lambda: ", folds_fit$lambda.min))
library(leaps)
library(glmnet)
# Generate data
set.seed(0)
X = c(rnorm(100))
e = rnorm(100, mean=0, sd=0.25)
# Specify variables
b_0 = 1
b_1 = 1.2
b_2 = 2
b_3 = 3
Y = c(b_0 + b_1*X + b_2*X^2 + b_3*X^3 + e)
df = data.frame(X,X^2,X^3,X^4,X^5,X^6,X^7,X^8,X^9,X^10,Y)
# Backward step wise selection
# Find best subsets
fit <- regsubsets(Y ~ ., data = df, method = "backward", nvmax=10)
summary(fit)
# Extract cp, bic and adjr2
cp = summary(fit)$cp
bic = summary(fit)$bic
adjusted_r_sq = summary(fit)$adjr2
# print coefficients
coef(fit, id=which.min(cp))
# Plot Cp
{plot(cp,
main="Backward selection",
xlab="Predictors",
ylab="Mallows' Cp",
type="l")
abline(v=which.min(cp),
col="red",
lwd=3,
lty=2)}
print(paste("The Cp decreases and then plateaus at the lowest value of",
which.min(cp),
"predictors."))
# Plot BIC
{plot(bic,
main="Backward selection",
xlab="Predictors",
ylab="Bayesian Information Criterion (BIC)",
type="l")
abline(v=which.min(bic),
col="red",
lwd=3,
lty=2)}
print(paste("The BIC decreases and then slowly increases as the number of predictors increases as well with a lowest BIC value at",
which.min(bic),
"predictors."))
# Plot adjr2
{plot(adjusted_r_sq,
main="Backward selection",
xlab="Predictors",
ylab="Adjusted R Squares",
type="l")
abline(v=which.max(adjusted_r_sq),
col="red",
lwd=3,
lty=2)}
print(paste("The Adjusted R Squares increases and then plateaus at the highest value of",
which.max(adjusted_r_sq),
"predictors. However, the Adjusted R Squares does not increase much after 3 predictors."))
print("Therefore the first 3 predictors (X, X^2, X^3) seem top be the best predictors to include when using backward selection.")
poly(X, degree = 10, raw = T) - poly(X, degree = 10, raw = T)
#forward stepwise selection
modelfit_fwd <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10,
method = "forward")
data=df
#forward stepwise selection
modelfit_fwd <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10,
method = "forward")
results_fwd <- summary(modelfit_fwd)
#backward stepwise selection
modelfit_bwd <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10,
method = "backward")
results_bwd <- summary(modelfit_bwd)
View(results_bwd)
View(modelfit_fwd)
View(modelfit_bwd)
View(fit)
View(modelfit_bwd)
# Backward step wise selection
# Find best subsets
fit <- regsubsets(Y ~ ., data = df, method = "backward", nvmax=10)
summary(fit)
# Extract cp, bic and adjr2
cp = summary(fit)$cp
bic = summary(fit)$bic
adjusted_r_sq = summary(fit)$adjr2
# print coefficients
coef(fit, id=which.min(cp))
# Plot Cp
{plot(cp,
main="Backward selection",
xlab="Predictors",
ylab="Mallows' Cp",
type="l")
abline(v=which.min(cp),
col="red",
lwd=3,
lty=2)}
print(paste("The Cp decreases and then plateaus at the lowest value of",
which.min(cp),
"predictors."))
# Plot BIC
{plot(bic,
main="Backward selection",
xlab="Predictors",
ylab="Bayesian Information Criterion (BIC)",
type="l")
abline(v=which.min(bic),
col="red",
lwd=3,
lty=2)}
print(paste("The BIC decreases and then slowly increases as the number of predictors increases as well with a lowest BIC value at",
which.min(bic),
"predictors."))
# Plot adjr2
{plot(adjusted_r_sq,
main="Backward selection",
xlab="Predictors",
ylab="Adjusted R Squares",
type="l")
abline(v=which.max(adjusted_r_sq),
col="red",
lwd=3,
lty=2)}
print(paste("The Adjusted R Squares increases and then plateaus at the highest value of",
which.max(adjusted_r_sq),
"predictors. However, the Adjusted R Squares does not increase much after 3 predictors."))
print("Therefore the first 3 predictors (X, X^2, X^3) seem top be the best predictors to include when using backward selection.")
#forward stepwise selection
modelfit_fwd <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10,
method = "forward")
results_fwd <- summary(modelfit_fwd)
#backward stepwise selection
modelfit_bwd <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10,
method = "backward")
results_bwd <- summary(modelfit_bwd)
par(mfrow = c(2,2))
plot(results_bwd$cp, xlab = "The Number of Variables", ylab = "Cp", type = "l")
points(which.min(results_bwd$cp), results_bwd$cp[which.min(results_bwd$cp)],
col = "red",
cex = 2, pch = 20)
plot(results_bwd$bic, xlab = "The Number of Variables", ylab = "BIC", type = "l")
points(which.min(results_bwd$bic), results_bwd$bic[which.min(results_bwd$bic)],
col = "red",
cex = 2, pch = 20)
plot(results_bwd$adjr2, xlab = "The Number of Variables", ylab = "Adjusted R2",
type = "l")
points(which.max(results_bwd$adjr2), results_bwd$adjr2[which.max(results_bwd$adjr2)],
col = "red",
cex = 2, pch = 20)
par(mfrow = c(1,1))
n=100
set.seed(0)
X <- rnorm(n)
noise <- rnorm(n)
beta0 = 1
beta1 = 1.2
beta2 = 2
beta3 = 3
Y <- beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + noise
library(leaps)
library(knitr)
library(pander)
#best subset selection
data <- data.frame(cbind(X,Y))
modelfit <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10)
results <- summary(modelfit)
which.min(results$cp)
which.min(results$bic)
which.max(results$adjr2)
#difference in the adjusted R2
results$adjr2[which.min(results$cp)] - results$adjr2[which.max(results$adjr2)]
par(mfrow = c(2,2))
plot(results$cp, xlab = "The Number of Variables", ylab = "Cp", type = "l")
points(which.min(results$cp), results$cp[which.min(results$cp)], col = "red",
cex = 2, pch = 20)
plot(results$bic, xlab = "The Number of Variables", ylab = "BIC", type = "l")
points(which.min(results$bic), results$bic[which.min(results$bic)], col = "red",
cex = 2, pch = 20)
plot(results$adjr2, xlab = "The Number of Variables", ylab = "Adjusted R2", type = "l")
points(which.max(results$adjr2), results$adjr2[which.max(results$adjr2)], col = "red",
cex = 2, pch = 20)
par(mfrow = c(1,1))
t <- data.frame(coef(modelfit, which.min(results$cp)))
colnames(t) <- "Est. coefficient"
rownames(t) <- c("Intercept", "X", "X_square", "X_cubic")
kable(t)
#forward stepwise selection
modelfit_fwd <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10,
method = "forward")
results_fwd <- summary(modelfit_fwd)
#backward stepwise selection
modelfit_bwd <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, nvmax = 10,
method = "backward")
results_bwd <- summary(modelfit_bwd)
par(mfrow = c(2,2))
plot(results_fwd$cp, xlab = "The Number of Variables", ylab = "Cp", type = "l")
points(which.min(results_fwd$cp), results_fwd$cp[which.min(results_fwd$cp)],
col = "red",
cex = 2, pch = 20)
plot(results_fwd$bic, xlab = "The Number of Variables", ylab = "BIC", type = "l")
points(which.min(results_fwd$bic), results_fwd$bic[which.min(results_fwd$bic)],
col = "red",
cex = 2, pch = 20)
plot(results_fwd$adjr2, xlab = "The Number of Variables", ylab = "Adjusted R2",
type = "l")
points(which.max(results_fwd$adjr2), results_fwd$adjr2[which.max(results_fwd$adjr2)],
col = "red",
cex = 2, pch = 20)
par(mfrow = c(1,1))
par(mfrow = c(2,2))
plot(results_bwd$cp, xlab = "The Number of Variables", ylab = "Cp", type = "l")
points(which.min(results_bwd$cp), results_bwd$cp[which.min(results_bwd$cp)],
col = "red",
cex = 2, pch = 20)
plot(results_bwd$bic, xlab = "The Number of Variables", ylab = "BIC", type = "l")
points(which.min(results_bwd$bic), results_bwd$bic[which.min(results_bwd$bic)],
col = "red",
cex = 2, pch = 20)
plot(results_bwd$adjr2, xlab = "The Number of Variables", ylab = "Adjusted R2",
type = "l")
points(which.max(results_bwd$adjr2), results_bwd$adjr2[which.max(results_bwd$adjr2)],
col = "red",
cex = 2, pch = 20)
par(mfrow = c(1,1))
t2 <- data.frame(coef(modelfit_fwd, which.min(results_fwd$cp)))
colnames(t2) <- "Est. coefficient"
rownames(t2) <- c("Intercept", "X", "X_square", "X_cubic")
kable(t2)
t3 <- data.frame(coef(modelfit_bwd, which.min(results_bwd$cp)))
colnames(t3) <- "Est. coefficient"
rownames(t3) <- c("Intercept", "X", "X_square", "X_5", "X_7", "X_9")
kable(t3)
library(glmnet)
#lasso
X_matrix <- poly(X, degree = 10, raw = TRUE)
#10-fold cross-validation
set.seed(0)
train_index <- sample(1:n, size = n/2)
cv.out <- cv.glmnet(X_matrix[train_index, ], Y[train_index],
alpha = 1, nfolds = 10)
#test MSE is
lasso_predict <- predict(cv.out, s = cv.out$lambda.min, newx = X_matrix[-train_index, ])
mean((lasso_predict - Y[-train_index])^2)
plot(cv.out)
round(cv.out$lambda.min, 3)
modelfit_lasso <- glmnet(X_matrix, Y, alpha = 1)
results_lasso <- predict(modelfit_lasso, type = "coefficients", s = cv.out$lambda.min)
results_lasso
#new generation
beta7 = 1.1
Y <- beta0 + beta7*X^7+noise
data2 <- data.frame(cbind(Y, X))
#the best subset selection
new_modelfit <- regsubsets(Y ~ poly(X, degree = 10, raw = TRUE), data = data2)
new_results <- summary(new_modelfit)
which.min(new_results$cp)
which.min(new_results$bic)
which.max(new_results$adjr2)
new_results$adjr2[which.min(new_results$cp)] - new_results$adjr2[which.max(new_results$adjr2)]
#The difference in the adjusted R2 between
#the one-variable model and the four-variable model is small
#best model
t <- data.frame(coef(new_modelfit, which.min(new_results$cp)))
colnames(t) <- "Est. coefficient"
rownames(t) <- c("Intercept", "X7")
kable(t)
#lasso
cv.out <- cv.glmnet(X_matrix[train_index,], Y[train_index], alpha = 1)
new_modelfit_lasso <- glmnet(X_matrix, Y, alpha = 1)
lasso.coef <- predict(new_modelfit_lasso, type = "coefficients", s = cv.out$lambda.min)
lasso.coef
# Backward step wise selection
# Find best subsets
fit <- regsubsets(Y ~ ., data = df, method = "backward", nvmax=10)
summary(fit)
# Extract cp, bic and adjr2
cp = summary(fit)$cp
bic = summary(fit)$bic
adjusted_r_sq = summary(fit)$adjr2
# print coefficients
coef(fit, id=which.min(cp))
# Plot Cp
{plot(cp,
main="Backward selection",
xlab="Predictors",
ylab="Mallows' Cp",
type="l")
abline(v=which.min(cp),
col="red",
lwd=3,
lty=2)}
print(paste("The Cp decreases and then plateaus at the lowest value of",
which.min(cp),
"predictors."))
# Plot BIC
{plot(bic,
main="Backward selection",
xlab="Predictors",
ylab="Bayesian Information Criterion (BIC)",
type="l")
abline(v=which.min(bic),
col="red",
lwd=3,
lty=2)}
print(paste("The BIC decreases and then slowly increases as the number of predictors increases as well with a lowest BIC value at",
which.min(bic),
"predictors."))
# Plot adjr2
{plot(adjusted_r_sq,
main="Backward selection",
xlab="Predictors",
ylab="Adjusted R Squares",
type="l")
abline(v=which.max(adjusted_r_sq),
col="red",
lwd=3,
lty=2)}
print(paste("The Adjusted R Squares increases and then plateaus at the highest value of",
which.max(adjusted_r_sq),
"predictors. However, the Adjusted R Squares does not increase much after 3 predictors."))
print("Therefore the first 3 predictors (X, X^2, X^3) seem top be the best predictors to include when using backward selection.")
# Backward step wise selection
# Find best subsets
fit <- regsubsets(Y ~ ., data = data, method = "backward", nvmax=10)
# Backward step wise selection
# Find best subsets
fit <- regsubsets(Y ~ poly(X, degree = 10, raw = T), data = data, method = "backward", nvmax=10)
summary(fit)
# Extract cp, bic and adjr2
cp = summary(fit)$cp
bic = summary(fit)$bic
adjusted_r_sq = summary(fit)$adjr2
# print coefficients
coef(fit, id=which.min(cp))
# Plot Cp
{plot(cp,
main="Backward selection",
xlab="Predictors",
ylab="Mallows' Cp",
type="l")
abline(v=which.min(cp),
col="red",
lwd=3,
lty=2)}
print(paste("The Cp decreases and then plateaus at the lowest value of",
which.min(cp),
"predictors."))
# Plot BIC
{plot(bic,
main="Backward selection",
xlab="Predictors",
ylab="Bayesian Information Criterion (BIC)",
type="l")
abline(v=which.min(bic),
col="red",
lwd=3,
lty=2)}
print(paste("The BIC decreases and then slowly increases as the number of predictors increases as well with a lowest BIC value at",
which.min(bic),
"predictors."))
# Plot adjr2
{plot(adjusted_r_sq,
main="Backward selection",
xlab="Predictors",
ylab="Adjusted R Squares",
type="l")
abline(v=which.max(adjusted_r_sq),
col="red",
lwd=3,
lty=2)}
print(paste("The Adjusted R Squares increases and then plateaus at the highest value of",
which.max(adjusted_r_sq),
"predictors. However, the Adjusted R Squares does not increase much after 3 predictors."))
print("Therefore the first 3 predictors (X, X^2, X^3) seem top be the best predictors to include when using backward selection.")
View(data)
View(df)
data_df = df$X
data_df$Y = df$Y
View(data_df)
data_df = data.frame(df$X)
data_df$Y = df$Y
View(data_df)
