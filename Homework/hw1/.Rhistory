miss_class_lstKNN = lapply(class_pred_lstKNN, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=k_grid,
y=miss_class_lstKNN,
main="miss-classifcation by radius",
xlab="Neighbours (k)",
ylim = c(0,1),
ylab="Miss classification rate",
col="red"
)
lines(x=k_grid,
y=miss_class_lstKNN,
col="red")
# get value for lowest point
knn_lowest_mcr = getMissClassRate(true = valid_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=14,
data=train_df,
func_type="knn"
))
print(c(r_lowest_mcr, knn_lowest_mcr))
##########
#2
##########
#####
#2.1
#####
#2.2
#####
#2.3
##########
#3
##########
#####
#3.1
set.seed(0)
n=100
lambda=10
x=rexp(n,lambda)
x
llhd=function(lamb){
theta=-n*log(lamb)+lamb*sum(x)
return(theta)}
lambda.mle=optimize(f=llhd,interval=c(0.1,20))$minimum
lambda.mle
#####
#3.2
"Take the likelihood function of exponential distribution, and take the natural
log of it. Then, take the first derivative with respect to lambda,
and set it equal to zero. We find the MLE estimator of lambda is n/Sigma x.
Take the second derivative and check if it is less than zero to make sure it
is the maximum instead of a minimum. "
lambda.mle2=n/sum(x)
lambda.mle2
##########
#4
##########
#####
#4.1
n=1000;size=4
CI1=matrix(0,n,2);CI2=matrix(0,n,2);CI3=matrix(0,n,2);coverage=matrix(0,n,3)
width1=rep(0,n);width2=rep(0,n);width3=rep(0,n)
for(j in 1:n){
set.seed(j)
x=rnorm(size,0,1)
CI1[j,]=mean(x)+c(-1,1)*qnorm(0.975)/sqrt(size)
CI2[j,]=mean(x)+c(-1,1)*qt(0.975,size-1)*sd(x)/sqrt(size)
CI3[j,]=mean(x)+c(-1,1)*qnorm(0.975)*sd(x)/sqrt(size)
if(CI1[j,1]<=0 & CI1[j,2]>=0) {coverage[j,1]=1}
if(CI2[j,1]<=0 & CI2[j,2]>=0) {coverage[j,2]=1}
if(CI3[j,1]<=0 & CI3[j,2]>=0) {coverage[j,3]=1}
width1[j]=CI1[j,2]-CI1[j,1]
width2[j]=CI2[j,2]-CI2[j,1]
width3[j]=CI3[j,2]-CI3[j,1]
}
freq.of.coverage=apply(coverage,2,mean)
freq.of.coverage
#####
#4.2
par(mfrow=c(1,2),mar=c(4,4,2,2))
hist(width2,probability = TRUE, xlab="width", ylab="propotion", main="case2")
hist(width3,probability = TRUE, xlab="width", ylab="propotion", main="case3")
# select best r for range of r values
# get class predictions
r_grid = seq(0.01, 0.15, 0.001)
class_pred_lst = lapply(r_grid,
getClass1Prediction,
x=valid_df,
t=0.5,
data=train_df,
func_type="radius")
# get miss classification rate
miss_class_lst = lapply(class_pred_lst, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=r_grid,
y=miss_class_lst,
main="miss-classifcation by radius",
xlab="Radius (r)",
ylab="Miss classification rate",
ylim=c(0,1),
col="blue"
)
lines(x=r_grid,
y=miss_class_lst,
col="blue")
print(c(r_lowest_mcr, knn_lowest_mcr))
########################################
# HOMEWORK 1 2022-09-16
########################################
##########
#1
##########
# import data and break up into train/validation/test sets
setwd("E:/GIT_REPOS/Classes/SML/hw1")
data_df <- read.csv("SML.NN.data.csv")
train_df = data_df[data_df$set == 'train',]
valid_df = data_df[data_df$set == 'valid',]
test_df = data_df[data_df$set == 'test',]
#####
#1.1
# function to get proportion of class 1 in radius to point x (single)
# output is a float
getClass1Prop <- function(x, r, data=train_df) {
"Import the data this function is
based on before using it. The data should be named data_df
and contain columns Y, X1, X2, set all in a dataframe.
x is a vector of length 2
r is the selected radius
"
# calculate the distance between the point and all the data
x1 = (data$X1-as.numeric(x[1]))^2
x2 = (data$X2-as.numeric(x[2]))^2
data$euc_dist = sqrt(rowSums(data.frame(x1,x2)))
#select points inside radius
in_r_df = data[data$euc_dist <= r,]
if(nrow(in_r_df)==0){
# return NA if no points within radius
return(NA)
} else {
# calculate proportion of class 1 values in data
class_1_prop = sum(in_r_df$Y)/nrow(data)
return(class_1_prop)
}
}
#####
#1.2
# get class 1 prediction for points in x (multiple)
# output is a vector of binary predictions
getClass1Prediction <- function(x, rk, t=0.5, data=train_df, func_type="radius"){
"t is the threshold and should be between 0 and 1."
pred_vec = c()
# loop through all points in x
for(i in 1:nrow(x)){
x_i = x[c('X1','X2')][i,] # get coordinates
if(func_type=="radius"){
class_1_prop = getClass1Prop(x=x_i, r=rk, data=data)
}else if(func_type=="knn") {
class_1_prop = getClass1PropKNN(x=x_i, k=rk, data=data)
}
if(is.na(class_1_prop)){
pred=NA
}
else if(class_1_prop >= t){
pred=1
}else{
pred=0
}
pred_vec=c(pred_vec, pred)
}
return(pred_vec)
}
# function to get confusion matrix of binary prediction
# output is a data frame in format:
# (true positive, false negative)
# (false positive, true negative)
# getConfusionMatrix <-function(true, pred){
#   "true is the real class of the data
#   pred is the predicted class of the same data"
#   df = data.frame(true, pred)
#   # calculate equality of data
#   df$equal = (df$true == df$pred)
#   # calculate tp, tn, fp, and fn
#   tp = as.numeric(nrow(df[(df$equal == TRUE) & (df$pred == 1),]))
#   tn = as.numeric(nrow(df[(df$equal == TRUE) & (df$pred == 0),]))
#   fp = as.numeric(nrow(df[(df$equal == FALSE) & (df$pred == 1),]))
#   fn = as.numeric(nrow(df[(df$equal == FALSE) & (df$pred == 0),]))
#   # save values in confusion matrix data frame
#   conf_mat_df = setNames(
#     data.frame(
#       c(tp, fp),
#       c(fn, tn),
#       row.names = c('1','0')),
#     c('1','0'))
#   return(conf_mat_df)
# }
#
# # function to get the
# getMissClassRate <-function(true, pred){
#   " Input is true vector and prediction vector
#   output is a ratio of wrong classifications.
#   "
#   conf_mat_df = getConfusionMatrix(true, pred)
#   miss_class_rate = (conf_mat_df['1','0'] + conf_mat_df['0','1'])/sum(conf_mat_df)
#   return(miss_class_rate)
# }
getMissClassRate <- function(true, pred){
# make dataframe of predictions and true values
df = data.frame(true, pred)
# calculate equality of data
df$equal = (df$true == df$pred)
mis_class_rate = 1 - sum(df$equal, na.rm = TRUE)/nrow(df)
return(mis_class_rate)
}
#####
#1.3
# make plot of coordinates density distribution
plot(train_df[train_df$Y==1,]$X1,
train_df[train_df$Y==1,]$X2,
main = "Distribution of classes",
xlab = "X1",
xlim=c(-1.1, 1.1),
ylab = "X2",
ylim=c(-1.1, 1.1),
pch = 15,
col = "blue")
points(train_df[train_df$Y==0,]$X1, train_df[train_df$Y==0,]$X2, pch = 0, col = "blue")
points(valid_df[valid_df$Y==1,]$X1, valid_df[valid_df$Y==1,]$X2, pch = 19, col = "red")
points(valid_df[valid_df$Y==0,]$X1, valid_df[valid_df$Y==0,]$X2, pch = 1, col = "red")
legend("topleft",
legend=c("Train class 1", "Train class 0", "Valid class 1", "Valid class 0"),
col=c("blue","blue","red","red"),
pch=c(15,0,19,1),
cex=0.8)
# 0.25 seems like a good number for r?
# the smaller the better? but too small and NAs become prevalent.
#####
#1.4
# select best r for range of r values
# get class predictions
r_grid = seq(0.01, 0.15, 0.001)
class_pred_lst = lapply(r_grid,
getClass1Prediction,
x=valid_df,
t=0.5,
data=train_df,
func_type="radius")
# get miss classification rate
miss_class_lst = lapply(class_pred_lst, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=r_grid,
y=miss_class_lst,
main="miss-classifcation by radius",
xlab="Radius (r)",
ylab="Miss classification rate",
ylim=c(0,1),
col="blue"
)
lines(x=r_grid,
y=miss_class_lst,
col="blue")
#####
#1.6
# KNN
getClass1PropKNN <- function(x, k, data=train_df) {
"Import the data this function is
based on before using it. The data should be named data_df
and contain columns Y, X1, X2, set all in a dataframe.
x is a vector of length 2
k is the number of neigbours
"
# calculate the distance between the point and all the data
x1 = (data$X1-as.numeric(x[1]))^2
x2 = (data$X2-as.numeric(x[2]))^2
data$euc_dist = sqrt(rowSums(data.frame(x1,x2)))
data = data[order(data$euc_dist),]
class_1_prop = sum(data$Y[1:k])/k
return(class_1_prop)
}
# get class predictions
k_grid = seq(1, 15, 1)
class_pred_lstKNN = lapply(k_grid,
getClass1Prediction,
x=valid_df,
t=0.5,
data=train_df,
func_type="knn")
# get miss classification rate
miss_class_lstKNN = lapply(class_pred_lstKNN, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=k_grid,
y=miss_class_lstKNN,
main="miss-classifcation by radius",
xlab="Neighbours (k)",
ylim = c(0,1),
ylab="Miss classification rate",
col="red"
)
lines(x=k_grid,
y=miss_class_lstKNN,
col="red")
# get value for lowest point
knn_lowest_mcr = getMissClassRate(true = valid_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=14,
data=train_df,
func_type="knn"
))
print(c(r_lowest_mcr, knn_lowest_mcr))
##########
#2
##########
#####
#2.1
#####
#2.2
#####
#2.3
##########
#3
##########
#####
#3.1
set.seed(0)
n=100
lambda=10
x=rexp(n,lambda)
x
llhd=function(lamb){
theta=-n*log(lamb)+lamb*sum(x)
return(theta)}
lambda.mle=optimize(f=llhd,interval=c(0.1,20))$minimum
lambda.mle
#####
#3.2
"Take the likelihood function of exponential distribution, and take the natural
log of it. Then, take the first derivative with respect to lambda,
and set it equal to zero. We find the MLE estimator of lambda is n/Sigma x.
Take the second derivative and check if it is less than zero to make sure it
is the maximum instead of a minimum. "
lambda.mle2=n/sum(x)
lambda.mle2
##########
#4
##########
#####
#4.1
n=1000;size=4
CI1=matrix(0,n,2);CI2=matrix(0,n,2);CI3=matrix(0,n,2);coverage=matrix(0,n,3)
width1=rep(0,n);width2=rep(0,n);width3=rep(0,n)
for(j in 1:n){
set.seed(j)
x=rnorm(size,0,1)
CI1[j,]=mean(x)+c(-1,1)*qnorm(0.975)/sqrt(size)
CI2[j,]=mean(x)+c(-1,1)*qt(0.975,size-1)*sd(x)/sqrt(size)
CI3[j,]=mean(x)+c(-1,1)*qnorm(0.975)*sd(x)/sqrt(size)
if(CI1[j,1]<=0 & CI1[j,2]>=0) {coverage[j,1]=1}
if(CI2[j,1]<=0 & CI2[j,2]>=0) {coverage[j,2]=1}
if(CI3[j,1]<=0 & CI3[j,2]>=0) {coverage[j,3]=1}
width1[j]=CI1[j,2]-CI1[j,1]
width2[j]=CI2[j,2]-CI2[j,1]
width3[j]=CI3[j,2]-CI3[j,1]
}
freq.of.coverage=apply(coverage,2,mean)
freq.of.coverage
#####
#4.2
par(mfrow=c(1,2),mar=c(4,4,2,2))
hist(width2,probability = TRUE, xlab="width", ylab="propotion", main="case2")
hist(width3,probability = TRUE, xlab="width", ylab="propotion", main="case3")
print(c(r_lowest_mcr, knn_lowest_mcr))
# get value for lowest point
r_lowest_mcr = getMissClassRate(true = valid_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=0.12,
data=train_df,
func_type="radius"
))
print(c(r_lowest_mcr, knn_lowest_mcr))
# get miss classification for test data with r*
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=0.12,
data=train_df,
func_type="radius"
))
# get miss classification for test data with guessed r
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=0.1,
data=train_df,
func_type="radius"
))
# get class predictions
k_grid = seq(1, 15, 1)
class_pred_lstKNN = lapply(k_grid,
getClass1Prediction,
x=valid_df,
t=0.5,
data=train_df,
func_type="knn")
# get miss classification rate
miss_class_lstKNN = lapply(class_pred_lstKNN, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=k_grid,
y=miss_class_lstKNN,
main="miss-classifcation by radius",
xlab="Neighbours (k)",
ylim = c(0,1),
ylab="Miss classification rate",
col="red"
)
lines(x=k_grid,
y=miss_class_lstKNN,
col="red")
# select best r for range of r values
# get class predictions
r_grid = seq(0.01, 0.15, 0.01)
class_pred_lst = lapply(r_grid,
getClass1Prediction,
x=valid_df,
t=0.5,
data=train_df,
func_type="radius")
# get miss classification rate
miss_class_lst = lapply(class_pred_lst, getMissClassRate, true = valid_df$Y)
# plot results
plot(x=r_grid,
y=miss_class_lst,
main="miss-classifcation by radius",
xlab="Radius (r)",
ylab="Miss classification rate",
ylim=c(0,1),
col="blue"
)
lines(x=r_grid,
y=miss_class_lst,
col="blue")
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=14,
data=train_df,
func_type="knn"
))
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=15,
data=train_df,
func_type="knn"
))
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=14,
data=train_df,
func_type="knn"
))
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=15,
data=train_df,
func_type="knn"
))
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=14,
data=train_df,
func_type="knn"
))
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=1,
data=train_df,
func_type="knn"
))
# get miss classification for test data with best k
getMissClassRate(true = test_df$Y,
pred = getClass1Prediction(
x=valid_df,
t=0.5,
rk=14,
data=train_df,
func_type="knn"
))
setwd("E:/GIT_REPOS/Classes/SML/hw1")
tinytex::install_tinytex()
