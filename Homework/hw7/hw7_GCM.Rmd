---
title: "STA6703 SML HW7"
 author: "Christopher Marais"
output: pdf_document
fontsize: 11pt
geometry: margin=2cm
---

# Chapter 7
## Problem 1
### a.)
  $$ f_1{(x)}= \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3$$

  - Where, $a_1=\beta_0$, $b_1=\beta_1$, $c_1=\beta_2$, $d_1=\beta_3$

### b.)
  $$\begin{aligned}
  f_2{(x)} &= \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x-\xi)^3 \\
  &= \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4(x^3-3x^2\xi+3x\xi^2-\xi^3)\\
  &= (\beta_0-\beta_4\xi^3) + x(\beta_1+3\beta_4\xi^2) + x^2(\beta_2-3\beta_4\xi) + x^3(\beta_3+\beta_4)   
  \end{aligned}
  $$
  
  - Where, $a_2=\beta_0-\beta_4\xi^3$, $b_2=\beta_1+3\beta_4\xi^2$, $c_2=\beta_2-3\beta_4\xi$, $d_2=\beta_3+\beta_4$

### c.)
  $$f_1(\xi) = \beta_0+\beta_1\xi+\beta_2\xi^2+\beta_3\xi^3$$
  $$\begin{aligned}
  f_2(\xi) &= (\beta_0-\beta_4\xi^3) + \xi(\beta_1+3\beta_4\xi^2) + \xi^2(\beta_2-3\beta_4\xi) + \xi^3(\beta_3+\beta_4) \\
  &=  \beta_0-\beta_4\xi^3+ \beta_1\xi+ 3\beta_4\xi^3 + \beta_2\xi^2-3\beta_4\xi^3+\beta_3\xi^3+\beta_4\xi^3 \\
  &= \beta_0+ \beta_1\xi+\beta_2\xi^2+\beta_3\xi^3 = f_1(\xi)
  \end{aligned}$$

### d.)
  $$f_1'(\xi) = \beta_1+2\beta_2\xi+3\beta_3\xi^2$$
  
  $$\begin{aligned}
  f_2'(\xi) &= (\beta_1+3\beta_4\xi^2) + 2\xi(\beta_2-3\beta_4\xi) + 3\xi^2(\beta_3+\beta_4) \\
  &=  \beta_1 + 3\beta_4\xi^2 + 2\beta_2\xi - 6\beta_4\xi^2 + 3\beta_3\xi^2 + 3\beta_4\xi^2 \\
  &=  \beta_1 + 2\beta_2\xi + 3\beta_3\xi^2 = f_1'(\xi)
  \end{aligned}$$

### e.)
  $$f_1''(\xi) = 2\beta_2+6\beta_3\xi$$
  
  $$\begin{aligned}
  f_2''(\xi) &= 2(\beta_2-3\beta_4\xi) + 6\xi(\beta_3+\beta_4) \\
  &=  2\beta_2 - 6\beta_4\xi + 6\beta_3\xi + 6\beta_4\xi \\
  &=  2\beta_2 + 3\beta_3\xi^2 = f_1''(\xi)
  \end{aligned}$$
  

## Problem 2
- In general, when $\lambda=\infty$, the penalty term is so large that it forces a function $g$ chosen to minimize the RSS into being perfectly smooth. This is because the penalty term reduces the variability in $g$.
### a.)
- When $\lambda=\infty$, $g = 0$. So, $\hat{g}=0$. 
  
### b.)
- When $\lambda=\infty$, $g'= 0$ (slope=0). So, $\hat{g}=constant$(say a horizontal line).

### c.)
- When $\lambda=\infty$, $g''= 0$ (the change in slope=0).So, $\hat{g}$ must be a straight line with a slope, say g_hat= cx + d.

### d.)
- When $\lambda=\infty$, $g'''= 0$(change in second derivative=0). So, $\hat{g}$ must be a quadratic curve, say g_hat= cx^2 + dx + e.

### e.)
- When $\lambda=0$ the penalty term has no effect, so we get a curve that interpolates all the n points perfectly (RSS Train = 0).

## Problem 3
```{R}
X = seq(-2,2,0.1)
Y = rep(NA,length(X))
for (i in 1:length(X)){
  if (X[i]<1){
    Y[i] = 1 + 1*X[i]
  }
  else{
    Y[i] = 1 + 1*X[i] - 2*(X[i]-1)^2
  }
}
plot(X,Y,type='l')
abline(h=0);abline(v=0);abline(v = 1, col = "red")
grid()
```

  - The curve is linear when $-2<X\leqslant1$, this portion has a slope and y intercept of 1. The curve then takes a quadratic shape when $1<X\leqslant2$.

## Problem 6
### a.)
```{R}
# Cross validation to choose degree of polynomial.
set.seed(1)
cv.error.10 = rep(0,10)
for (i in 1:10) {
  glm.fit=glm(wage~poly(age,i),data=Wage)
  cv.error.10[i]=cv.glm(Wage,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error.10, type="b", xlab="Degree", ylab="CV Error")
```
- The CV errors does not show clear improvement after degree 4 polynomial.

```{R}
lm.fit = glm(wage~poly(age,4),data=Wage)
summary(lm.fit)
```

```{R}
# Using Anova() to compare degree 4 model with others.
fit.1 = lm(wage~age ,data=Wage)
fit.2 = lm(wage~poly(age ,2) ,data=Wage)
fit.3 = lm(wage~poly(age ,3) ,data=Wage)
fit.4 = lm(wage~poly(age ,4) ,data=Wage)
fit.5 = lm(wage~poly(age ,5) ,data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
```
  
  - The p-values comparing lower order models up to degree 3 are statistically significant.The p-value comparing model 3 to 4 is slightly above 5%, whereas that comparing 4 to 5 is not statistically significant. The results show a cubic or quartic model as providing the best fit, with higher or lower order polynomials being unjustified.
  
  - The results match that of polynomial regression using 4 degrees. 

```{R}
# Grid of values for age at which we want predictions.
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
# Predictions.
preds=predict(lm.fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
# Plot of polynomial fit onto data including SE bands.
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Polynomial fit using degree 4")
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd =1,col="blue",lty =3)
```

### b.)
```{R}
# Cross validation to choose optimal number of cuts.
set.seed(1)
cv.error.20 = rep(NA,19)
for (i in 2:20) {
  Wage$age.cut = cut(Wage$age,i)
  step.fit=glm(wage~age.cut,data=Wage)
  cv.error.20[i-1]=cv.glm(Wage,step.fit,K=10)$delta[1] # [1]: Std [2]: Bias corrected.
}
cv.error.20
plot(cv.error.20,type='b',ylab="CV Error")
```
  - The data and chart shows that the CV errors do not improve substantially after 8 (Index+1) cuts, and so 8 cuts will be used to fit the step function.

```{R}
step.fit = glm(wage~cut(age,8), data=Wage)
preds2=predict(step.fit,newdata=list(age=age.grid), se=T)
se.bands2=cbind(preds2$fit+2*preds2$se.fit,preds2$fit-2*preds2$se.fit)
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Step function using 8 cuts")
lines(age.grid,preds2$fit,lwd=2,col="blue")
matlines(age.grid,se.bands2,lwd =1,col="blue",lty =3)
```

## Problem 10
### a.)
```{R}
set.seed(4)
#sum(is.na(College$Outstate))
college_df = College
college_sample = sample.split(college_df$Outstate, SplitRatio = 0.80)
college_train = subset(college_df, college_sample==TRUE) 
college_test = subset(college_df, college_sample==FALSE)
```


__Forward stepwise selection to identify a satisfactory model:__
```{r results='show',warning=FALSE}
# Forward stepwise on the training set using all variables.
fit.fwd = regsubsets(Outstate~., data=college_train, nvmax=17, method="forward")
fit.summary = summary(fit.fwd)
# Some Statistical metrics.
which.min(fit.summary$cp)    #Estimate of the test error, lower is better.
which.min(fit.summary$bic)   #Takes a small value for models with low test errors, so lower is better generally.
which.max(fit.summary$adjr2) #A larger value indicates a model with low test error.
par(mfrow=c(2,2))
plot(1:17, fit.summary$cp,xlab="Variables",ylab="Cp",main="Cp", type='b')
plot(1:17, fit.summary$bic,xlab="Variables",ylab="BIC",main="BIC", type='b')
plot(1:17, fit.summary$adjr2,xlab="Variables",ylab="Adjusted R2",main="Adjusted R2", type='b')
```
  - The Cp, BIC and Adjusted R^2 all identify minimums and a maximum for models with a different number of variables. As can be seen from the charts, the metrics change rapidly as the number of variables increase, but there are only small improvements after a model with 6 variables.
  
  - Therefore, the model with 6 variables is selected as it appears to be satisfactory in describing this relationship.
  
```{R}
coef(fit.fwd,6)
```

### b.)
```{R}
gam.m1 = gam(Outstate~Private+
               s(Room.Board,4)+
               s(PhD,4)+
               s(perc.alumni,2)+
               s(Expend,4)+
               s(Grad.Rate,5), data=college_train)
par(mfrow=c(2,3))
plot(gam.m1, col="blue", se=T)
```
  - Holding other variables fixed, out of state tuition increases as room and board costs get higher.
  - Similarly, out of state tuition increases as the proportion of alumni who donate increase.

### c.)
```{R}
# Predictions and MSE on Outstate from test set.
preds = predict(gam.m1,newdata = college_test)
mse = mean((college_test$Outstate - preds)^2)
mse
```
  
```{R}
# Graduation rate appears to have a non-linear relationship with Outstate. We can confirm this by performing an ANOVA test.
gam.m2 = gam(Outstate~Private+s(Room.Board,4)+s(PhD,4)+s(perc.alumni,2)+s(Expend,4), data=college_train) # No Grad.Rate
gam.m3 = gam(Outstate~Private+s(Room.Board,4)+s(PhD,4)+s(perc.alumni,2)+s(Expend,4)+Grad.Rate, data=college_train) # Linear Grad rate
gam.m4 = gam(Outstate~Private+s(Room.Board,4)+s(PhD,4)+s(perc.alumni,2)+s(Expend,4)+s(Grad.Rate,4), data=college_train) # Spline with 4 degrees
anova(gam.m2,gam.m3,gam.m4,gam.m1, test="F")
```
  - The results provide compelling evidence that a GAM which includes `Grad.Rate` as a non-linear function(spline with degree 4) is needed(p=0.03939).
