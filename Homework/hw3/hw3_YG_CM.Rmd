---
title: "SML HW3"
author: "Yujia Guo & Christopher Marais"
output: pdf_document
fontsize: 11pt
geometry: margin=2cm
---
# Problem 1

```{R}
x1 <- c(6,2,9,1)
x2 <- c(2,4,1,2)
x3 <- c(4,-2,8,-1)
m1 <- cbind(x1,x2,x3)
lmod <- lm(x1~x3+x2)
summary(lmod)
```
x1, x2, x3 are linearly dependent. Regress x1 on x2 and x3, we will get that the R-squared equals to 1 and RSS=0. Also, since x1 is depend on both x1 and x2, the regression p-value of x2 and x3 are both less than 0.05. In general, A set of vectors {x1, x2, · · · , xp} is linearly dependent if there exist numbers β1, β2, · · · βp
,not all equal to zero, such that
β1x1 + β2x2 + · · · + βpxP = 0
It can be re-written as
x1 = −(β2x2 + · · · + βpxP )/β1
Therefore, we can get a perfect fit and the rss will be 0.

# Problem 2

```{R}
m2 <- matrix(rnorm(10000* 2), # Create random matrix
ncol = 2)
cor(m2[,1],m2[,2])
```
The correlation of this sample is very closed to 0. We can conclude that in general, if numerical predictors are linearly independent, then they are uncorrelated.

# Problem 3

## 3.1 
```{R}
myOLS <- function(Y, X, is1 = TRUE) {
# Inputs:
# * Y is the vector of length n of response variables
# * X is an n-by-p matrix of numerical covariates (in columns); p < n
# ** assume the columns of X are linearly independent and
# ** do not include the column for the intercept as a part of the X matrix
# * is1 is a logical "flag" whether the intercept is included; is1 = TRUE by default
# Output:
# the function must return a list L with two elements:
# L[1] will contain the vector of OLS/MLE coefficients, betahat
# L[2] will contain standard errors (i.e., estimated standard deviations) for betahat

  n_y = length(Y)
  n_x = nrow(X)
  if(n_x != n_y){
    print("Error: X and Y not of same length")
  }else{
    n = n_y
  }
  
  p = ncol(X)
  
  if(is1==FALSE){
    # add intercept to matrix
    X0 = rep(1, n)
    X = cbind(X0, X)
  }
  
  betahat = solve(t(X)%*%X)%*%t(X)%*%Y
  pred = X%*%betahat # prediction
  sigma_sq <- sum((Y - pred)^2)/(n-p)  # estimate of sigma-squared
  var_covar <- sigma_sq*solve(t(X)%*%X) # variance covariance matrix
  std_err <- sqrt(diag(var_covar)) # standard error
  
  return(list(betahat, std_err))
}

```

```{R}
n = 30
set.seed(0)
p = 3
X = matrix(runif(n*p),nrow=n)*2-1
b = seq(1,p,by=1)
Y = X%*%b + rnorm(n)
fit1 = lm(Y ~ X); summary(fit1)

```

```{R}
myOLS(Y,X,FALSE)
```
## 3.2 
```{R}
myPolyReg1 <- function(Y, X1, deg=1) {
# Inputs: same as for myOLS, except
# * X1 is a vector of length n that contain the covariate values (numerical)
# * deg is the degree k (i.e., largest power) of the polynomial fit; k < n ; deg=1 by default.
# Outputs: same as for myOLS
  X = rep(1, n)
  for (i in seq(1, deg)){
    X = cbind(X, X1**i)
  }
  
  return(myOLS(X=X, Y=Y, is1=TRUE))
}
```

```{R}
n = 30 
set.seed(0)
X = runif(n)*4-2 # X is uniformly distributed on [-2,2]
Y = 1 + 3*X -2*X^2 + 1*X^3 + rnorm(n)
fit0 = lm(Y ~ X + I(X^2) + I(X^3))
summary(fit0)
```

```{R}
myPolyReg1(Y,X,deg=3)
```
## 3.3
```{R}
myAnova1 <- function(Y, XF, is1=TRUE) {
# Inputs: same as for myOLS, except
# * XF is a vector of length n that contain the covariate values (categorical or "factor")
# Outputs: same as for myOLS
  uniq_var <- unique(XF)
  X <- +outer(XF, uniq_var, `==`)
  colnames(X) <- uniq_var
  
  if(is1==FALSE){
    X = X[,-1]
  }
  return(myOLS(X=X, Y=Y, is1=is1))
}
```

```{R}
n = 30
set.seed(0)
XF = rep(c("A","B","C"),each=10)
Y = rnorm(n) + rep(c(1,2,3),each=10)
fit1 = lm(Y ~ XF)
summary(fit1) # with an intercept
```

```{R}
myAnova1(Y, XF, is1=FALSE)
```

```{R}
fit0 = lm(Y ~ -1 + XF)
summary(fit0) # without an intercept
```

```{R}
myAnova1(Y, XF, is1=TRUE)
```
# Problem 4
```{R}
n = 30 
set.seed(0)
X = runif(n)*4-2
Y = 1 + 3*X + rnorm(n)
fit1 = lm(Y ~ X) 
summary(fit1) # beta0_hat = 1.0161; beta1_hat = 2.9304
```

## 4.1
```{R}
myFullObj <- function(par) {
# Inputs:
# * b is the vector of regression coefficients, b=[b0,b1];
# * sig is the standard dev of errors; sig > 0
# Output: the negative log-likelihood of the observed data (Y given X) evaluated at b and sig
  b = par[1:2]
  sig = par[3]
  miu <- b[1]+mean(X)*b[2]
  return((-1)*sum(dnorm(Y, mean=miu, sd = sig, log=TRUE)))
}
```

## 4.2 
```{R}
for(i in c(2,0.1,1,10,100)){
  sigKnown = i
  myObj1 <- function(b){
    myFullObj(c(b,sigKnown))
  }
  print(c("Sigknown = ", i))
  print(optim(par=c(1,3),myObj1,method="BFGS"))
  print("----------------")
}

```
For all the tested values of sigma (0.1, 1, 10, 100) it produces very similar results with beta0_hat always close to 1 and beta1_hat always very close to 3. The negative log-likelihood for each of these differ though. This means that no matter the standard deviation it is always able to minimize the function to the same beta parameters.

These results are very similar to the ones produced by the lm() function. This means that both methods converge at a similar answer for a linear regression model that describes the data. This is likely because the estimated line is the best fit to the data.  

## 4.3 
```{R}
optim(par = c(1,3,0), myFullObj,method="L-BFGS-B",lower=c(-Inf, -Inf, 10^(-5)))
```
For different values of beta0_hat and beta1_hat the sigma value stays constant at approximately 3.56. When optim() is initialized with different values of beta0_hat and beta1_hat it converges at different estimates of beta but the sigma estimate stays the same. This means that there are multiple values of beta0_hat and beta1_hat that are optimal but only one optimum for sigma. 