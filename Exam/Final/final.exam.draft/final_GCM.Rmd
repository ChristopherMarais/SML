---
title: "ABE6933 SML Take-Home Final Exam (100 pts + 10 pts bonus)"
author: "Christopher Marais"
output: pdf_document
fontsize: 11pt
geometry: margin=2cm
---
# Exam code & data
```{R}
myCVids <- function(n, K, seed=0) {
# balanced subsets generation (subset sizes differ by at most 1)
# n is the number of observations/rows in the training set
# K is the desired number of folds (e.g., 5 or 10)
set.seed(seed);
t = floor(n/K); r = n-t*K;
id0 = rep((1:K),times=t)
ids = sample(id0,t*K)
if (r > 0) {ids = c(ids, sample(K,r))}
ids
}

# function to generate all subsets of the set (1,2,...,p)
myf <- function(p) {
  out = matrix(c(0,1),nrow=2);
  if (p > 1) {
    for (i in (1:(p-1))) {
      d = 2^i
      o1 = cbind(rep(0,d),out)
      o2 = cbind(rep(1,d),out)
      out = rbind(o1,o2)
    }
  }
  colnames(out) <- c(2^((p-1):0)); # powers for binary expansion
  # colnames(out) <- c()
  out
}

nbSubsets <- function(p,m) {
  M  = myf(p)
  rs = rowSums(M)
  ii = (rs == m)
  (M[ii,])
}

# function to convert binary representation to decimal representation 
bin2dec <- function(binM) {
  dd = dim(binM);  # nrows and ncols
  p = dd[2]-1      # max power; 
  d = rep(0,dd[1]) # initialize placeholder for the answer
  for (i in 1:(p+1)) {
    d = d + 2^(p+1-i)*binM[,i]
  }
  d
}

load('final.test.v1.Rdata')
```


# Problem 1
## 1.1
```{R}

```

## 1.2
```{R}

```

## 1.3
```{R}

```

## 1.4
```{R}

```

## 1.5
```{R}

```

## 1.6
```{R}

```

# Problem 2
```{R}

```

# Problem 3
```{R}
# initialize information
k = 5
d_max = 10

# Mode function
Mode <- function(x) {
  ox <- order(x)
  ux <- unique(ox)
  ux[which.max(tabulate(match(x, ux)))]
}

# Outer CV to estimate performance (seed=0)
n_i = nrow(prob3.df)
for(k_i in seq(k)){
  inds.part = myCVids(n=n_i, K=k, seed=0)
  isk = (inds.part == k_i)
  valid.i = which(isk)
  train.i = which(!isk)
  # split data into train and test sets
  data.valid.i = prob3.df[valid.i,]
  rownames(data.valid.i) <- NULL
  data.train.i = prob3.df[train.i,]
  rownames(data.train.i) <- NULL
  
  # Inner CV to estimate parameters (seed=1000)
  n_j = nrow(data.train.i)
  d_opt_vec = c()
  mse_opt_vec = c()
  for(k_j in seq(k)){
    inds.part = myCVids(n=n_j, K=k, seed=1000)
    isk = (inds.part == k_j)
    valid.j = which(isk)
    train.j = which(!isk)
    # split data into train and test sets
    data.valid.j = data.train.i[valid.j,]
    data.train.j = data.train.i[train.j,]

    # test different parameters (d)
    lm_results <- lapply(1:d_max, function(d) lm(y ~ poly(x, d), data = data.train.j))
    mse_vec = c()
    for(d in seq(d_max)){
      lm.fit.j = lm_results[[d]]
      mse_j = mean((data.valid.j$y - predict(lm.fit.j , data.valid.j))^2)
      mse_vec = c(mse_vec, mse_j)
    }
    d_opt = which.min(mse_vec)
    mse_opt = min(mse_vec)
    d_opt_vec = c(d_opt_vec, d_opt)
    mse_opt_vec = c(mse_opt_vec, mse_opt)
    
    d_mse_opt_df = data.frame(d_opt_vec, mse_opt_vec)
  }
  
  # select the mode of parameter d for all inner folds
  # if multiple modes, select the smallest value for d
  mode_d = Mode(d_mse_opt_df$d_opt_vec)
  
  # use mode_d to retrain polynomial model on data in outer CV
  lm.fit.i = lm(y ~ poly(x, mode_d), data = data.train.i)
  mse_i = mean((data.valid.i$y - predict(lm.fit.i , data.valid.i))^2)
  
  # estiamte performance
  print(paste(mode_d, mse_i))
}

# should model be trained in outer and inner loop? yes
# potential drawbacks are computation time
# Report mse and d for outer loop
# report mse and d for inner loop
# group them together print dataframe and then final d and final mse for each outer loop
```

```{R}

```

# Problem 4
## 4.a
```{R}
# initialize data
p = 4

binM = myf(p)
ids = bin2dec(binM)

# edit this line so it works
glm(Y ~ X[,gamma==1], family=binomial, data=prob4.df)

# perform 5fold CV

# plot of the CV estimates of the test misclassification error 
# rate versus model id, as well as the
# selected covariates in the best model

ids
```

## 4.b
```{R}

```

# Problem 5
```{R}

```

# Problem 6
```{R}

```
